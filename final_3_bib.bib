@article{madley-dowd_proportion_2019,
abstract = {Objectives
Researchers are concerned whether multiple imputation (MI) or complete case analysis should be used when a large proportion of data are missing. We aimed to provide guidance for drawing conclusions from data with a large proportion of missingness.
Study Design and Setting
Via simulations, we investigated how the proportion of missing data, the fraction of missing information (FMI), and availability of auxiliary variables affected MI performance. Outcome data were missing completely at random or missing at random (MAR).
Results
Provided sufficient auxiliary information was available; MI was beneficial in terms of bias and never detrimental in terms of efficiency. Models with similar FMI values, but differing proportions of missing data, also had similar precision for effect estimates. In the absence of bias, the FMI was a better guide to the efficiency gains using MI than the proportion of missing data.
Conclusion
We provide evidence that for MAR data, valid MI reduces bias even when the proportion of missingness is large. We advise researchers to use FMI to guide choice of auxiliary variables for efficiency gain in imputation analyses, and that sensitivity analyses including different imputation models may be needed if the number of complete cases is small.},
author = {Madley-Dowd, Paul and Hughes, Rachael and Tilling, Kate and Heron, Jon},
doi = {10.1016/j.jclinepi.2019.02.016},
file = {:home/marius/Documents/Books/H{\o}st 2019/Articles/MI articles/Already read/***The proportion of missing data should not be used to guide decisions onmultiple imputation.pdf:pdf},
issn = {0895-4356},
journal = {Journal of Clinical Epidemiology},
keywords = {ALSPAC,Bias,Methods,Missing data,Multiple imputation,Simulation},
pages = {63--73},
title = {{The proportion of missing data should not be used to guide decisions on multiple imputation}},
url = {http://www.sciencedirect.com/science/article/pii/S0895435618308710},
volume = {110},
year = {2019}
}
@article{schafer_multiple_2016,
abstract = {In recent years, multiple imputation has emerged as a convenient and flexible paradigm for analysing data with missing values. Essential features of multiple im...},
author = {Schafer, Joseph L},
doi = {10.1177/096228029900800102},
journal = {Statistical Methods in Medical Research},
shorttitle = {Multiple imputation},
title = {{Multiple imputation: a primer:}},
url = {https://journals.sagepub.com/doi/10.1177/096228029900800102},
year = {2016}
}
@article{rubin_multiple_1996,
abstract = {Multiple imputation was designed to handle the problem of missing data in public-use data bases where the data-base constructor and the ultimate user are distinct entities. The objective is valid frequency inference for ultimate users who in general have access only to complete-data software and possess limited knowledge of specific reasons and models for nonresponse. For this situation and objective, I believe that multiple imputation by the data-base constructor is the method of choice. This article first provides a description of the assumed context and objectives, and second, reviews the multiple imputation framework and its standard results. These preliminary discussions are especially important because some recent commentaries on multiple imputation have reflected either misunderstandings of the practical objectives of multiple imputation or misunderstandings of fundamental theoretical results. Then, criticisms of multiple imputation are considered, and, finally, comparisons are made to alternative strategies. Copyright 1996 Taylor {\&} Francis Group, LLC.},
annote = {Cited By :1909},
author = {Rubin, D B},
doi = {10.1080/01621459.1996.10476908},
journal = {Journal of the American Statistical Association},
keywords = {Confidence validity,Missing data,Nonresponse in surveys,Public-use files,Sample surveys,Superefficient procedures},
number = {434},
pages = {473--489},
title = {{Multiple {\{}Imputation{\}} after 18+ {\{}Years{\}}}},
volume = {91},
year = {1996}
}
@article{Sterne2009,
abstract = {We are enthusiastic about the potential for multiple imputation and other methods 14 to improve the validity of medical research results and to reduce the waste of resources caused by missing data. The cost of multiple imputation analyses is small compared with the cost of collecting the data. It would be a pity if the avoidable pitfalls of multiple imputation slowed progress towards the wider use of these methods. It is no longer excusable for missing values and the reason they arose to be swept under the carpet, nor for potentially misleading and inefficient analyses of complete cases to be considered adequate. We hope that the pitfalls and guidelines discussed here will contribute to the appropriate use and reporting of methods to deal with missing data.},
author = {Sterne, Jonathan A.C. and White, Ian R. and Carlin, John B. and Spratt, Michael and Royston, Patrick and Kenward, Michael G. and Wood, Angela M. and Carpenter, James R.},
doi = {10.1136/bmj.b2393},
file = {:home/marius/Documents/Books/H{\o}st 2019/Articles/Sterneetal-BMJ2009-Multiple{\_}imputation{\_}for{\_}missin.pdf:pdf},
issn = {17561833},
journal = {BMJ (Online)},
number = {7713},
pages = {157--160},
pmid = {19564179},
title = {{Multiple imputation for missing data in epidemiological and clinical research: Potential and pitfalls}},
volume = {339},
year = {2009}
}
@article{Schafer2002,
abstract = {Statistical procedures for missing data have vastly improved, yet misconception and unsound practice still abound. The authors frame the missing-data problem, review methods offer advice, and raise issues that remain unresolved. They clear up common misunderstandings regarding the missing at random (MAR) concept. They summarize the evidence against older procedures and, with few exceptions, discourage their use. They present, in both technical and practical language, 2 general approaches that come highly recommended: maximum likelihood (ML) and Bayesian multiple imputation (MI). Newer developments are discussed, including some for dealing with missing data that are not MAR. Although not yet in the mainstream, these procedures may eventually extend the ML and MI methods that currently represent the state of the art.},
author = {Schafer, Josepn L. and Graham, John W.},
doi = {10.1037/1082-989X.7.2.147},
file = {:home/marius/Documents/Books/H{\o}st 2019/Articles/SchaferAndGraham2002.pdf:pdf},
issn = {1082989X},
journal = {Psychological Methods},
number = {2},
pages = {147--177},
pmid = {12090408},
title = {{Missing data: Our view of the state of the art}},
volume = {7},
year = {2002}
}
@article{Xie2017,
abstract = {Real-life data are almost never really real. By the time the data arrive at an investigator's desk or disk, the raw data, however defined, have most likely gone through at least one "cleaning" process, such as standardization, re-calibration, imputation, or de-sensitization. Dealing with such a reality scientifically requires a more holistic multi-phase perspective than is permitted by the usual framework of "God's model versus my model." This article provides an in-depth look, from this broader perspective, into multiple-imputation (MI) inference (Rubin (1987)) under uncongeniality (Meng (1994)). We present a general estimating-equation decomposition theorem, resulting in an analytic (asymptotic) description of MI inference as an integration of the knowledge of the imputer and the analyst, and establish a characterization of self-efficiency (Meng (1994)) for regulating estimation procedures. These results help to reveal how the quality of and relationship between the imputer's model and analyst's procedure affect MI inference, including how a seemingly perfect procedure under the "God-versus-me" paradigm is actually inadmissible when God's, imputer's, and analyst's models are uncongenial to each other. Our theoretical investigation also leads to useful procedures that are as trivially implementable as Rubin's combining rules, yet with confidence coverage guaranteed to be minimally the nominal level, under any degree of uncongeniality. We reveal that the relationship is very complex between the validity of approaches taken for individual phases and the validity of the final multi-phase inference, and indeed that it is a nontrivial matter to quantify or even qualify the meaning of validity itself in such settings. These results and many open problems are presented to raise the general awareness that the multi-phase inference paradigm is an uncongenial forest populated by thorns, as well as some fruits, many of which are still low-hanging.},
author = {Xie, Xianchao and Meng, Xiao Li},
doi = {10.5705/ss.2014.067},
file = {:home/marius/Downloads/Articles for citation/A27n41-10.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {Confidence validity,Data cleaning,Estimating equation decomposition,Incomplete data,Multi-phase inference,Pre-processing,Self-efficiency,Strong efficiency,Uncongeniality},
number = {4},
pages = {1485--1545},
title = {{Dissecting multiple imput a tion from a multi-phase inference perspective: What happens when god's, imputer's and anal Yst's models are uncongenial?}},
volume = {27},
year = {2017}
}
@article{KenwardM.G.andMolenberghs2009,
author = {{Kenward, M. G. and Molenberghs}, G.},
journal = {Journal of Biopharmaceutical Statistics},
pages = {872--888},
title = {{Last observation carried forward: A crystal ball?}},
volume = {19},
year = {2009}
}
@article{Meng1994,
author = {Meng, Xiao-Li},
journal = {Statistical Science1},
pages = {538--558},
title = {{Multiple-imputation inferences with uncongenial sources of input.}},
volume = {9},
year = {1994}
}
@article{Schafer1999,
abstract = {In recent years, multiple imputation has emerged as a convenient and flexible paradigm for analysing data with missing values. Essential features of multiple imputation are reviewed, with answers to frequently asked questions about using the method in practice.},
author = {Schafer, Joseph L.},
doi = {10.1191/096228099671525676},
file = {:home/marius/Downloads/Articles for citation/Multiple imputation$\backslash$: a primer.pdf:pdf},
issn = {09622802},
journal = {Statistical Methods in Medical Research},
number = {1},
pages = {3--15},
title = {{Multiple imputation: A primer}},
volume = {8},
year = {1999}
}
@article{Lee,
abstract = {Statistical analysis in epidemiologic studies is often hindered by missing data, and multiple imputation is increasingly being used to handle this problem. In a simulation study, the authors compared 2 methods for imputation that are widely available in standard software: fully conditional specification (FCS) or ''chained equations'' and multivariate normal imputation (MVNI). The authors created data sets of 1,000 observations to simulate a cohort study, and missing data were induced under 3 missing-data mechanisms. Imputations were performed using FCS (Royston's ''ice'') and MVNI (Schafer's NORM) in Stata (Stata Corporation, College Station, Texas), with transformations or prediction matching being used to manage nonnormality in the continuous variables. Inferences for a set of regression parameters were compared between these approaches and a complete-case analysis. As expected, both FCS and MVNI were generally less biased than complete-case analysis, and both produced similar results despite the presence of binary and ordinal variables that clearly did not follow a normal distribution. Ignoring skewness in a continuous covariate led to large biases and poor coverage for the corresponding regression parameter under both approaches, although inferences for other parameters were largely unaffected. These results provide reassurance that similar results can be expected from FCS and MVNI in a standard regression analysis involving variously scaled variables. data interpretation; statistical; epidemiologic methods; imputation; incomplete data; missing data; simulations Abbreviations: FCS, fully conditional specification; MVNI, multivariate normal imputation.},
author = {Lee, Katherine J and Carlin, John B},
doi = {10.1093/aje/kwp425},
file = {:home/marius/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Carlin - Unknown - Practice of Epidemiology Multiple Imputation for Missing Data Fully Conditional Specification Versus Multivariat.pdf:pdf},
number = {5},
title = {{Practice of Epidemiology Multiple Imputation for Missing Data: Fully Conditional Specification Versus Multivariate Normal Imputation}},
url = {https://academic.oup.com/aje/article-abstract/171/5/624/137388},
volume = {171}
}
@book{Little2019,
address = {Hoboken, NJ},
author = {Little, Roderick J. A. and Rubin, Donald B.},
edition = {Third},
editor = {{David J. Balding, Noel A. C. Cressie}, Garrett M. Fitzmaurice and {Geof H. Givens, Harvey Goldstein, Geert Molenberghs}, David W. Scott and {Adrian F. M. Smith}, Ruey S. Tsay},
pages = {464},
publisher = {Wiley},
title = {{Statistical Analysis with Missing Data, Third Edition}},
year = {2019}
}
@article{Rubin1976,
author = {Rubin, Donald B.},
journal = {Biometrika},
pages = {581--592},
title = {{Inference and missing data}},
volume = {63},
year = {1976}
}
@book{Molenberghs2015,
address = {Boca Raton, FL},
author = {Molenberghs, Geert and Fitzmaurice, Garrett and Kenward, Michael G. and Tsiatis, Anastasios and Verbeke, Geert},
pages = {574},
publisher = {Chapman and Hall/CRC},
title = {{Handbook of Missing Data Methodology}},
year = {2015}
}
